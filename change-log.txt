This file describes the following:
A. The log of changes I made to the repo.
B. The results of my efforts.
C. A summary of what else can be done to enable the API to scale to 1M requests per minute.


========================================================================================================================


A. First here is the log of the changes I made to this repo:

1. Add change-log.txt to the repo.

// Added MongoDB to the project.
2. Add mongodb driver to the package.
3. Added db.js file in the database folder to set up and connect to the mongodb.
4. Added README.txt file to the database folder describing how to set up the database using docker.
5. Updated POST /api/usages endpoint to save the data to the 'Usages' collection of the 'busyapi' mongodb database.
6. Added a GET /api/usages/count endpoint to return the number of documents in the 'Usages' collection.
7. Removed unneeded temporary storage for Usages.

// Set the project up for Docker containerization
8. Added a 'docker' folder.
9. Added a README.txt file in the docker folder that explains how to: build the docker image and spin up 4 instances of the API container.

// Set up the project for simple load balancing using nginx.
10. Added an 'nginx' folder.
11. Added a README.txt file in the nginx folder that explains how to start the nginx server which will load balance the busyapi API requests
    and dish them out to 4 busyapi API docker containers.

// Added info on how to test the load of the system using Bombardier
12. Added 'bombardier' folder.
13. Added README.txt file in the bombardier folder that explains how to use bombardier to load test the system locally.

14. Removed morgan logger and any other potential things that would write to the console because writing to the console is a blocking operation.
15. Set NODE_ENV to 'production'


========================================================================================================================================


B. Here is the results of my efforts.

So I did my best to make this scale to a massive amount of requests using a single computer with 4 CPU cores. My laptop
also doesn't have a Solid State Drive so yeah. Oh and I did all testing / programming inside of a linux virtual machine on
my Windows 10 laptop.

What I did was set up a mongoDb to store the Usage data because keeping the data in memory wasn't going to scale.
See database/README.txt for more detail

Then I turned the API into a docker image that could be spun up as many times as we need to in order to scale us to 1M requests
per minute. Then I spun up 4 containers because I have 4 CPU cores. See docker/README.txt for more info.

Then I set up a simple nginx server so we could have one API url to hit; namely 'localhost:3000'. And the nginx server will load
balance the requests between all 4 of the docker API containers. See nginx/README.txt for more info.

And so with all of that I now essentially have scaled up 4 API servers.

Then in order to test the load of the system to see how close I got to our 1M requests per minute, I used Bombardier: a HTTP
benchmarking tool. See bombardier/README.txt for more info.

The final test I ran had bombardier make 250,000 POST requests to localhost:3000/api/usages. And the result was that the server
fulfilled roughly 2067 requests per second which means roughly 124,000 requests per minute. Here is the actual output of the linux terminal:

        anthony@anthony-VirtualBox:~$ bombardier -c 1000 -n 250000 http://localhost:3000/api/usages -m POST --body='{"patientId":100,"timestamp":"Tue Nov 01 2016 09:11:51 GMT-0500 (CDT)","medication":"Albuterol"}'
        Bombarding http://localhost:3000/api/usages with 250000 request(s) using 1000 connection(s)
        250000 / 250000 [=================================================================================================] 100.00% 2m1s
        Done!
        Statistics        Avg      Stdev        Max
        Reqs/sec      2067.35    1007.52    7557.68
        Latency      482.70ms   433.80ms      8.19s
        HTTP codes:
            1xx - 0, 2xx - 250000, 3xx - 0, 4xx - 0, 5xx - 0
            others - 0
        Throughput:     1.01MB/s

So did I make it there (1M requests per minute) with my single 4 core laptop? Well no I didn't. But this should demonstrate to you that I am capable of
scaling and I have a way of testing whether my code can handle large amounts of requests. According to the data I would only need
roughly 10 equivalent set ups of my laptop to scale to 1M requests per minute. So no my laptop alone can't handle 1M requests per minute but in the 
next section I will discuss how we can scale this API using the pattern I demonstrated here on 1 physical PC.


====================================================================================================================================

C. Here is a discussion of how to use what I have started to fully scale this API up.

In the original assessment email there was some questions that should be included in the write up.
So the point of this section is to describe what can be done to fully scale this API up which is really
what the questions are about. So this section will take the form of a Q & A where the questions come from
the email.


Question 1: Suggested changes to the software architecture/stack that may achieve the goal
Answer:
   One thing that can be done is to store some of the Usages and after a bunch of Usages are ready to be saved
   then write to the database. So instead of each POST request resulting in an insert into the mongoDb what we could
   do wait until we store 1,000 Usage posts in memory or wait every 2 seconds (which ever one comes first) and then
   call a .insertMany to limit the number of overhead costs with database communication. This is possible
   because I do believe there is a way to generate a MongoDb Object Id on the API server. Thus we would be able to
   send back the Usage Id like we need to but we can wait and save it later using that same id.

   Another thing is that I noticed we do technically have some html that can be served up. I am viewing this more as
   an API than something that servers webpages. But one thing we will want to make sure is that the serving of webpages
   happens in its own server. This will minimize the number of web page requests and allow the server to focus on
   API requests.

   One moe thing that can be done is to ensure we are following the Express.js production guide. It includes a bunch
   of insight on what should be done before releasing the express.js server into a production environment. I did not
   have enough time to go through this guide and perform its recommendations. But one of them is setting NODE_ENV to
   production which I did do. http://expressjs.com/en/advanced/best-practice-performance.html

Question 2: Suggested changes to the physical architecture/hardware that may achieve the goal
Answer:
    First is that the API needs to be containerized. Which I did using docker. So I made the API into a docker image.
    This docker image can then be started up as many times, on as many servers as we need or want to achieve our goal.
    In order to scale this up to 1M requests per minute, I would recommend engaging cloud infrastructure. So I can talk
    about how that would look using Amazon Web Services. First we would have a server base image so that we could spin up
    as many linux servers as we would like. AWS calls this an EC2 base image. Then amazon can start up EC2 instances using
    that base image. Then amazon can spin up our API docker image on as many EC2 instances as we configure. Then we hook
    it up to Amazon's Elastic Load Balancer (ELB) which will then distribute the load of requests to the various server
    instances that we have set up. And then on top of that we can hook it up to Amazon's auto scaling feature so that
    we will only have as many server instances / docker API containers as we need for the given moment. Some times of
    day have higher traffic than others. And so setting up auto scaling triggers allows us to make sure we rise to the 
    challenge but not have too many resources always allocated. So in the end Amazon determines how many docker API
    containers are running and Amazon determines how many linux server instances are also running. Fully scalable,
    unlimited potential, able to handle 1M requests per minute and more!

    That section was just talking about scaling the API server code and not talking about the mongo database. So the
    second thing that will enable us to scale to 1M requests per minute and beyond is making sure our database can scale
    up as well. So I chose mongo db for the job because it is inherently able to work on a distributed system. Meaning
    mongo db supports more than one mongo server working together for the same database. So another suggestion would be
    to utilize a mongoDb service that allows you to have multiple mongoDb servers hookers up in a distributed system.
    This is made possible through sharding and replica sets.

    Another thing that would have made my PC perform better would have been to have a Solid State Drive.

Question 3. A working build of the code capable of achieving the goal
Answer:
    I believe I do have a working build of the code capable of achieving the goal of 1M requests per minute.
    I do have the code. It needs to be turned into a docker image (see docker/README.txt) but that image is
    capable of doing what you want. You just have to spin up enough linux servers and API containers using that
    image. Given my data I believe 10 linux servers running 4 API containers each would be able to do it. Probably
    less than 10 is even needed because my data is based off of my laptop which isn't dedicated to purely running
    the API server anyways (it is running Windows 10 and Ubuntu Linux, etc...).

Question 4. Methods of measuring whether or not the goal has been achieved
Answer:
    I used a basic benchmarking tool that would give me some quick insight into the request load that my code
    and tech stack is capable of. I used Bombardier for that (see bombardier/README.txt for more info).

    However a more sophisticated method would be needed in order to really load test the scaled up API set up.
    Especially for 1M requests per minute. I would recommend using some cloud testing tools which allows you to test this
    exact thing. You would spin up your servers and API docker containers and then set up the testing tool to
    hit the server via the specifications you gave it. Configure it for 1M requests per minute using x devices,
    etc... And then observe the testing tool's report. Also you can observe the resource charts of the servers for the
    duration of the tests. And don't forget to look at the Mongo Db reports of usage and resources as well.
    All of this will give you an idea of if the system is handling 1M requests per minute.
