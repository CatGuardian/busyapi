This file describes the following:
A. The log of changes I made to the repo.
B. The results of my efforts.
C. A summary of what else can be done to enable the API to scale to 1M requests per minute.


========================================================================================================================


A. First here is the log of the changes I made to this repo:

1. Add change-log.txt to the repo.

// Added MongoDB to the project.
2. Add mongodb driver to the package.
3. Added db.js file in the database folder to set up and connect to the mongodb.
4. Added README.txt file to the database folder describing how to set up the database using docker.
5. Updated POST /api/usages endpoint to save the data to the 'Usages' collection of the 'busyapi' mongodb database.
6. Added a GET /api/usages/count endpoint to return the number of documents in the 'Usages' collection.
7. Removed unneeded temporary storage for Usages.

// Set the project up for Docker containerization
8. Added a 'docker' folder.
9. Added a README.txt file in the docker folder that explains how to: build the docker image and spin up 4 instances of the API container.

// Set up the project for simple load balancing using nginx.
10. Added an 'nginx' folder.
11. Added a README.txt file in the nginx folder that explains how to start the nginx server which will load balance the busyapi API requests
    and dish them out to 4 busyapi API docker containers.

// Added info on how to test the load of the system using Bombardier
12. Added 'bombardier' folder.
13. Added README.txt file in the bombardier folder that explains how to use bombardier to load test the system locally.

14. Removed morgan logger and any other potential things that would write to the console because writing to the console is blocking.
15. Set NODE_ENV to 'production'


========================================================================================================================================


B. Here is the results of my efforts.

So I did my best to make this scale to a massive amount of requests using a single computer with 4 CPU cores. My laptop
also doesn't have a Solid State Drive so yeah. Oh and I did all testing / programming inside of a linux virtual machine on
my Windows 10 laptop.

What I did was set up a mongoDb to store the Usage data because keeping the data in memory wasn't going to scale.
See database/README.txt for more detail

Then I turned the API into a docker image that could be spun up as many times as we need to in order to scale us to 1M requests
per minute. Then I spun up 4 containers because I have 4 CPU cores. See docker/README.txt for more info.

Then I set up a simple nginx server so we could have one API url to hit; namely 'localhost:3000'. And the nginx server will load
balance the requests between all 4 of the docker API containers. See nginx/README.txt for more info.

And so with all of that I now essentially have scaled up 4 API servers.

Then in order to test the load of the system to see how close I got to our 1M requests per minute, I used Bombardier: a HTTP
benchmarking tool. See bombardier/README.txt for more info.

The final test I ran had bombardier make 250,000 POST requests to localhost:3000/api/usages. And the result was that the server
fulfilled roughly 2067 requests per second which means roughly 124,000 requests per minute. Here is the actual output of the linux terminal:

        anthony@anthony-VirtualBox:~$ bombardier -c 1000 -n 250000 http://localhost:3000/api/usages -m POST --body='{"patientId":100,"timestamp":"Tue Nov 01 2016 09:11:51 GMT-0500 (CDT)","medication":"Albuterol"}'
        Bombarding http://localhost:3000/api/usages with 250000 request(s) using 1000 connection(s)
        250000 / 250000 [=================================================================================================] 100.00% 2m1s
        Done!
        Statistics        Avg      Stdev        Max
        Reqs/sec      2067.35    1007.52    7557.68
        Latency      482.70ms   433.80ms      8.19s
        HTTP codes:
            1xx - 0, 2xx - 250000, 3xx - 0, 4xx - 0, 5xx - 0
            others - 0
        Throughput:     1.01MB/s

So did I make it there (1M requests per minute) with my single 4 core laptop? Well no I didn't. But this should demonstrate to you that I am capable of
scaling and I have a way of testing whether my code can handle large amounts of requests. According to the data I would only need
roughly 10 equivalent set ups of my laptop to scale to 1M requests per minute. So no my laptop alone can't handle 1M requests per minute but in the 
next section I will discuss how we can scale this API using the pattern I demonstrated here on 1 physical PC.


====================================================================================================================================

C. Here is a discussion of how to use what I have started to fully scale this API up.

In the original assessment email there was some questions that should be included in the write up.
So the point of this section is to describe what can be done to fully scale this API up which is really
what the questions are about. So this section will take the form of a Q & A where the questions come from
the email.


Question 1: Suggested changes to the software architecture/stack that may achieve the goal
Answer:
   One thing that can be done is to store some of the Usages and after a bunch of Usages are ready to be saved
   then write to the database. So instead of each POST request resulting in an insert into the mongoDb what we could
   do wait until we store 1,000 Usage posts in memory or wait every 2 seconds (which ever one comes first) and then
   call a .insertMany to limit the number of overhead costs with database communication. This is possible
   because I do believe there is a way to generate a MongoDb Object Id on the API server. Thus we would be able to
   send back the Usage Id like we need to but we can wait and save it later using that same id.

   Another thing is that I noticed we do technically have some html that can be served up. I am viewing this more as
   an API than something that servers webpages. But one thing we will want to make sure is that the serving of webpages
   happens in its own server. This will minimize the number of web page requests and allow the server to focus on
   API requests.

Question 2: Suggested changes to the physical architecture/hardware that may achieve the goal
Answer:
    First is that the API needs to be containerized. Which I did using docker. So I made the API into a docker image.
    This docker image can then be started up as many times, on as many servers as we need or want to achieve our goal.
    In order to scale this up to 1M requests per minute, I would recommend engaging cloud infrastructure. So I can talk
    about how that would look using Amazon Web Services. First we would have a server base image so that we could spin up
    as many linux servers as we would like. AWS calls this an EC2 base image. Then amazon can start up EC2 instances using
    that base image. Then amazon can spin up our API docker image on as many EC2 instances as we configure. Then we hook
    it up to Amazon's Elastic Load Balancer (ELB) which will then distribute the load of requests to the various server
    instances that we have set up. And then on top of that we can hook it up to Amazon's auto scaling feature so that
    we will only have as many server instances / docker API containers as we need for the given moment. Some times of
    day have higher traffic than others. And so setting up auto scaling triggers allows us to make sure we rise to the 
    challenge but not have too many resources always allocated. So in the end Amazon determines how many docker API
    containers are running and Amazon determines how many linux server instances are also running. Fully scalable,
    unlimited potential, able to handle 1M requests per minute and more!

    That section was just talking about scaling the API server code and not talking about the mongo database. So the
    second thing that will enable us to scale to 1M requests per minute and beyond is making sure our database can scale
    up as well. So I chose mongo db for the job because it is inherently able to work on a distributed system. Meaning
    mongo db supports more than one mongo server working together for the same database. 

A working build of the code capable of achieving the goal
Methods of measuring whether or not the goal has been achieved
